{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fada601",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Report' object has no attribute 'save_html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m report\u001b[38;5;241m.\u001b[39mrun(reference_data\u001b[38;5;241m=\u001b[39mreference_data, current_data\u001b[38;5;241m=\u001b[39mcurrent_data)\u001b[38;5;241m.\u001b[39msave_html(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_drift_report.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Save the report as HTML\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mreport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_html\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreports/data_drift_report.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Report' object has no attribute 'save_html'"
     ]
    }
   ],
   "source": [
    "from evidently import Report\n",
    "from evidently.presets import DataSummaryPreset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "reference_data = pd.read_csv(r\"/mnt/d/fsds_aws/data/processed/train/housing_train_processed.csv\")\n",
    "current_data = pd.read_csv(r\"/mnt/d/fsds_aws/data/processed/test/housing_test_processed.csv\")\n",
    "\n",
    "# Generate the report\n",
    "report = Report([DataSummaryPreset()])\n",
    "report.run(reference_data=reference_data, current_data=current_data).save_html(\"data_drift_report.html\")\n",
    "\n",
    "# Save the report as HTML\n",
    "# report.save_html(\"reports/data_drift_report.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589378a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<evidently.core.report.Report at 0x7fb3b17e3ac0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f08848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a496d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d852fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f634f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ColumnSummaryMetric' from 'evidently.metrics' (/home/sunilpradhan/conda/envs/fsds/lib/python3.10/site-packages/evidently/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# from evidently.metrics import ColumnDriftMetric\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnSummaryMetric\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDriftMetric\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetMissingValuesMetric\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ColumnSummaryMetric' from 'evidently.metrics' (/home/sunilpradhan/conda/envs/fsds/lib/python3.10/site-packages/evidently/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from evidently.metrics import ColumnDriftMetric\n",
    "from evidently.metrics import ColumnSummaryMetric\n",
    "from evidently.metrics import DatasetDriftMetric\n",
    "from evidently.metrics import DatasetMissingValuesMetric\n",
    "from evidently import Report\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.ui.dashboards import CounterAgg\n",
    "from evidently.ui.dashboards import DashboardPanelCounter\n",
    "from evidently.ui.dashboards import DashboardPanelPlot\n",
    "from evidently.ui.dashboards import PanelValue\n",
    "from evidently.ui.dashboards import PlotType\n",
    "from evidently.ui.dashboards import ReportFilter\n",
    "from evidently.ui.remote import RemoteWorkspace\n",
    "from evidently.ui.workspace import Workspace\n",
    "from evidently.ui.workspace import WorkspaceBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e50a029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reference',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'categorical_tests',\n",
       " 'eq',\n",
       " 'gt',\n",
       " 'gte',\n",
       " 'is_in',\n",
       " 'lt',\n",
       " 'lte',\n",
       " 'not_eq',\n",
       " 'not_in',\n",
       " 'numerical_tests',\n",
       " 'reference']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evidently\n",
    "dir(evidently.tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ba344a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataQualityPreset' from 'evidently.presets' (/home/sunilpradhan/conda/envs/fsds/lib/python3.10/site-packages/evidently/presets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Report\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpresets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataDriftPreset, DataQualityPreset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_preset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataDriftTestPreset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_suite\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TestSuite\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataQualityPreset' from 'evidently.presets' (/home/sunilpradhan/conda/envs/fsds/lib/python3.10/site-packages/evidently/presets/__init__.py)"
     ]
    }
   ],
   "source": [
    "from evidently import Report\n",
    "from evidently.presets import DataDriftPreset, DataQualityPreset\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.ui.remote import RemoteWorkspace\n",
    "from evidently.ui.workspace import Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97daeb12",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2230997547.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    evidently ui --demo-project\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "evidently ui --demo-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e04e9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.ui.workspace import Workspace\n",
    "from evidently.ui.workspace import WorkspaceBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29c85983",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = \"workspace\"\n",
    "\n",
    "YOUR_PROJECT_NAME = \"New Project\"\n",
    "YOUR_PROJECT_DESCRIPTION = \"Test project using Adult dataset.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600078b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WorkspaceBase.create_project() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m project \u001b[38;5;241m=\u001b[39m \u001b[43mWorkspaceBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mYOUR_PROJECT_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m project\u001b[38;5;241m.\u001b[39mdescription \u001b[38;5;241m=\u001b[39m YOUR_PROJECT_DESCRIPTION\n",
      "\u001b[0;31mTypeError\u001b[0m: WorkspaceBase.create_project() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "project = WorkspaceBase.create_project(name=YOUR_PROJECT_NAME)\n",
    "project.description = YOUR_PROJECT_DESCRIPTIFON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eed6aa69",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataQualityPreset' from 'evidently.presets' (/home/sunilpradhan/conda/envs/fsds/lib/python3.10/site-packages/evidently/presets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpresets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataDriftPreset, DataQualityPreset\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataQualityPreset' from 'evidently.presets' (/home/sunilpradhan/conda/envs/fsds/lib/python3.10/site-packages/evidently/presets/__init__.py)"
     ]
    }
   ],
   "source": [
    "from evidently.presets import DataDriftPreset, DataQualityPreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b60d5ae8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ColumnDriftMetric' from 'evidently.metrics' (/home/sunilpradhan/conda/envs/fsds/lib/python3.10/site-packages/evidently/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnDriftMetric\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnSummaryMetric\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDriftMetric\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ColumnDriftMetric' from 'evidently.metrics' (/home/sunilpradhan/conda/envs/fsds/lib/python3.10/site-packages/evidently/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from evidently.metrics import ColumnDriftMetric\n",
    "from evidently.metrics import ColumnSummaryMetric\n",
    "from evidently.metrics import DatasetDriftMetric\n",
    "from evidently.metrics import DatasetMissingValuesMetric\n",
    "from evidently.report import Report\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.ui.dashboards import CounterAgg\n",
    "from evidently.ui.dashboards import DashboardPanelCounter\n",
    "from evidently.ui.dashboards import DashboardPanelPlot\n",
    "from evidently.ui.dashboards import PanelValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6494e001",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evidently.calculations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalculations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstattests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StatTest\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evidently.calculations'"
     ]
    }
   ],
   "source": [
    "from evidently.calculations.stattests.registry import StatTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "293b7976",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evidently.future.calculations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuture\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalculations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstattests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StatTest\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_anderson_stat_test\u001b[39m(reference_data: pd\u001b[38;5;241m.\u001b[39mSeries, current_data: pd\u001b[38;5;241m.\u001b[39mSeries, feature_type: \u001b[38;5;28mstr\u001b[39m, threshold: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m      4\u001b[0m     p_value \u001b[38;5;241m=\u001b[39m anderson_ksamp(np\u001b[38;5;241m.\u001b[39marray([reference_data, current_data]))[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evidently.future.calculations'"
     ]
    }
   ],
   "source": [
    "from evidently.future.calculations.stattests import StatTest\n",
    "\n",
    "def _anderson_stat_test(reference_data: pd.Series, current_data: pd.Series, feature_type: str, threshold: float):\n",
    "    p_value = anderson_ksamp(np.array([reference_data, current_data]))[2]\n",
    "    return p_value, p_value < threshold\n",
    "\n",
    "anderson_stat_test = StatTest(\n",
    "    name=\"anderson\",\n",
    "    display_name=\"Anderson test (p_value)\",\n",
    "    func=_anderson_stat_test,\n",
    "    allowed_feature_types=[\"num\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_report = Report(metrics=[\n",
    "    RegressionQualityMetric(),\n",
    "    RegressionErrorPlot(),\n",
    "    RegressionErrorDistribution(),\n",
    "    DataDriftPreset(stattest=anderson_stat_test, stattest_threshold=0.9),\n",
    "])\n",
    "\n",
    "\n",
    "the_report.run(\n",
    "    reference_data=reference,\n",
    "    current_data=current.loc['2011-02-14 00:00:00':'2011-02-21 23:00:00'],\n",
    "    column_mapping=column_mapping_drift\n",
    ")\n",
    "the_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52331702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently import Report\n",
    "from evidently.metrics import *\n",
    "from evidently.presets import *\n",
    "from evidently.tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c90bde22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ColumnDriftMetric' from 'evidently.metrics' (/home/sunilpradhan/conda/envs/fsds/lib/python3.10/site-packages/evidently/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnDriftMetric\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnSummaryMetric\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDriftMetric\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ColumnDriftMetric' from 'evidently.metrics' (/home/sunilpradhan/conda/envs/fsds/lib/python3.10/site-packages/evidently/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from evidently.metrics import ColumnDriftMetric\n",
    "from evidently.metrics import ColumnSummaryMetric\n",
    "from evidently.metrics import DatasetDriftMetric\n",
    "from evidently.metrics import DatasetMissingValuesMetric\n",
    "from evidently.report import Report\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.ui.dashboards import CounterAgg\n",
    "from evidently.ui.dashboards import DashboardPanelCounter\n",
    "from evidently.ui.dashboards import DashboardPanelPlot\n",
    "from evidently.ui.dashboards import PanelValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88196e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from typing import Dict\n",
    "from typing import Generic\n",
    "from typing import List\n",
    "\n",
    "from evidently.core.metric_types import BoundTest\n",
    "from evidently.core.metric_types import MeanStdCalculation\n",
    "from evidently.core.metric_types import MeanStdMetric\n",
    "from evidently.core.metric_types import MeanStdValue\n",
    "from evidently.core.metric_types import SingleValue\n",
    "from evidently.core.metric_types import SingleValueCalculation\n",
    "from evidently.core.metric_types import SingleValueMetric\n",
    "from evidently.core.metric_types import TMeanStdMetric\n",
    "from evidently.core.metric_types import TSingleValueMetric\n",
    "from evidently.core.report import Context\n",
    "from evidently.core.report import _default_input_data_generator\n",
    "from evidently.legacy.base_metric import InputData\n",
    "from evidently.legacy.base_metric import Metric\n",
    "from evidently.legacy.metrics import RegressionAbsPercentageErrorPlot\n",
    "from evidently.legacy.metrics import RegressionDummyMetric\n",
    "from evidently.legacy.metrics import RegressionErrorDistribution\n",
    "from evidently.legacy.metrics import RegressionErrorNormality\n",
    "from evidently.legacy.metrics import RegressionErrorPlot\n",
    "from evidently.legacy.metrics import RegressionPredictedVsActualPlot\n",
    "from evidently.legacy.metrics.regression_performance.regression_dummy_metric import RegressionDummyMetricResults\n",
    "from evidently.legacy.metrics.regression_performance.regression_quality import RegressionQualityMetric\n",
    "from evidently.legacy.metrics.regression_performance.regression_quality import RegressionQualityMetricResults\n",
    "from evidently.legacy.model.widget import BaseWidgetInfo\n",
    "from evidently.legacy.utils.data_preprocessing import create_data_definition\n",
    "from evidently.metrics._legacy import LegacyMetricCalculation\n",
    "from evidently.tests import Reference\n",
    "from evidently.tests import eq\n",
    "from evidently.tests import gt\n",
    "from evidently.tests import lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc8488d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_WIDGET_MAPPING: Dict[str, Metric] = {\n",
    "    \"error_plot\": RegressionErrorPlot(),\n",
    "    \"error_distr\": RegressionErrorDistribution(),\n",
    "    \"error_normality\": RegressionErrorNormality(),\n",
    "    \"perc_error_plot\": RegressionAbsPercentageErrorPlot(),\n",
    "    \"pred_actual_plot\": RegressionPredictedVsActualPlot(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a87ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gen_regression_input_data(context: \"Context\") -> InputData:\n",
    "    default_data = _default_input_data_generator(context)\n",
    "    regression = context.data_definition.get_regression(\"default\")\n",
    "    if regression is None:\n",
    "        raise ValueError(\"No default regression in data definition\")\n",
    "    default_data.column_mapping.target = regression.target\n",
    "    default_data.column_mapping.prediction = regression.prediction\n",
    "\n",
    "    definition = create_data_definition(\n",
    "        default_data.reference_data,\n",
    "        default_data.current_data,\n",
    "        default_data.column_mapping,\n",
    "    )\n",
    "    default_data.data_definition = definition\n",
    "    return default_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad994513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegacyRegressionMeanStdMetric(\n",
    "    MeanStdCalculation[TMeanStdMetric],\n",
    "    LegacyMetricCalculation[MeanStdValue, TMeanStdMetric, RegressionQualityMetricResults, RegressionQualityMetric],\n",
    "    Generic[TMeanStdMetric],\n",
    "    abc.ABC,\n",
    "):\n",
    "    def legacy_metric(self) -> RegressionQualityMetric:\n",
    "        return RegressionQualityMetric()\n",
    "\n",
    "    def get_additional_widgets(self, context: \"Context\") -> List[BaseWidgetInfo]:\n",
    "        result = []\n",
    "        for field, metric in ADDITIONAL_WIDGET_MAPPING.items():\n",
    "            if hasattr(self.metric, field) and getattr(self.metric, field):\n",
    "                _, widgets = context.get_legacy_metric(metric, _gen_regression_input_data)\n",
    "                result += widgets\n",
    "        return result\n",
    "\n",
    "    def _gen_input_data(self, context: \"Context\") -> InputData:\n",
    "        return _gen_regression_input_data(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8741b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegacyRegressionSingleValueMetric(\n",
    "    SingleValueCalculation[TSingleValueMetric],\n",
    "    LegacyMetricCalculation[SingleValue, TSingleValueMetric, RegressionQualityMetricResults, RegressionQualityMetric],\n",
    "    Generic[TSingleValueMetric],\n",
    "    abc.ABC,\n",
    "):\n",
    "    def legacy_metric(self) -> RegressionQualityMetric:\n",
    "        return RegressionQualityMetric()\n",
    "\n",
    "    def get_additional_widgets(self, context: \"Context\") -> List[BaseWidgetInfo]:\n",
    "        result = []\n",
    "        for field, metric in ADDITIONAL_WIDGET_MAPPING.items():\n",
    "            if hasattr(self.metric, field) and getattr(self.metric, field):\n",
    "                _, widgets = context.get_legacy_metric(metric, _gen_regression_input_data)\n",
    "                result += widgets\n",
    "        return result\n",
    "\n",
    "    def _gen_input_data(self, context: \"Context\") -> InputData:\n",
    "        return _gen_regression_input_data(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ed7a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanError(MeanStdMetric):\n",
    "    error_plot: bool = True\n",
    "    error_distr: bool = False\n",
    "    error_normality: bool = False\n",
    "\n",
    "    def _default_tests_with_reference(self, context: Context) -> List[BoundTest]:\n",
    "        return [eq(Reference(relative=0.1)).bind_mean_std(self.get_fingerprint())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e52c1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanErrorCalculation(LegacyRegressionMeanStdMetric[MeanError]):\n",
    "    def calculate_value(\n",
    "        self, context: Context, legacy_result: RegressionQualityMetricResults, render: List[BaseWidgetInfo]\n",
    "    ):\n",
    "        return (\n",
    "            self.result(\n",
    "                legacy_result.current.mean_error,\n",
    "                legacy_result.current.error_std,\n",
    "            ),\n",
    "            None\n",
    "            if legacy_result.reference is None\n",
    "            else self.result(\n",
    "                legacy_result.reference.mean_error,\n",
    "                legacy_result.reference.error_std,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def display_name(self) -> str:\n",
    "        return \"Mean Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2de27bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(MeanStdMetric):\n",
    "    error_plot: bool = False\n",
    "    error_distr: bool = True\n",
    "    error_normality: bool = False\n",
    "\n",
    "    def _default_tests_with_reference(self, context: Context) -> List[BoundTest]:\n",
    "        return [eq(Reference(relative=0.1)).bind_mean_std(self.get_fingerprint(), True)]\n",
    "\n",
    "    def _default_tests(self, context: \"Context\") -> List[BoundTest]:\n",
    "        dv: SingleValue = context.calculate_metric(DummyMAE().to_calculation())\n",
    "        return [lt(dv.value).bind_mean_std(self.get_fingerprint())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67b26b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAECalculation(LegacyRegressionMeanStdMetric[MAE]):\n",
    "    def calculate_value(\n",
    "        self, context: Context, legacy_result: RegressionQualityMetricResults, render: List[BaseWidgetInfo]\n",
    "    ):\n",
    "        return (\n",
    "            self.result(\n",
    "                legacy_result.current.mean_abs_error,\n",
    "                legacy_result.current.abs_error_std,\n",
    "            ),\n",
    "            None\n",
    "            if legacy_result.reference is None\n",
    "            else self.result(\n",
    "                legacy_result.reference.mean_abs_error,\n",
    "                legacy_result.reference.abs_error_std,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def display_name(self) -> str:\n",
    "        return \"Mean Absolute Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1190a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSE(SingleValueMetric):\n",
    "    error_plot: bool = False\n",
    "    error_distr: bool = True\n",
    "    error_normality: bool = False\n",
    "\n",
    "    def _default_tests_with_reference(self, context: Context) -> List[BoundTest]:\n",
    "        return [eq(Reference(relative=0.1)).bind_single(self.get_fingerprint())]\n",
    "\n",
    "    def _default_tests(self, context: \"Context\") -> List[BoundTest]:\n",
    "        dv: SingleValue = context.calculate_metric(DummyRMSE().to_calculation())\n",
    "        return [lt(dv.value).bind_single(self.get_fingerprint())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da37550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSECalculation(LegacyRegressionSingleValueMetric[RMSE]):\n",
    "    def calculate_value(\n",
    "        self, context: Context, legacy_result: RegressionQualityMetricResults, render: List[BaseWidgetInfo]\n",
    "    ):\n",
    "        return (\n",
    "            self.result(legacy_result.current.rmse),\n",
    "            None if legacy_result.reference is None else self.result(legacy_result.reference.rmse),\n",
    "        )\n",
    "\n",
    "    def display_name(self) -> str:\n",
    "        return \"RMSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "98b78f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPE(MeanStdMetric):\n",
    "    perc_error_plot: bool = True\n",
    "    error_distr: bool = False\n",
    "\n",
    "    def _default_tests_with_reference(self, context: Context) -> List[BoundTest]:\n",
    "        return [eq(Reference(relative=0.1)).bind_mean_std(self.get_fingerprint())]\n",
    "\n",
    "    def _default_tests(self, context: \"Context\") -> List[BoundTest]:\n",
    "        dv: SingleValue = context.calculate_metric(DummyMAPE().to_calculation())\n",
    "        return [lt(dv.value).bind_mean_std(self.get_fingerprint())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8e7acf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPECalculation(LegacyRegressionMeanStdMetric[MAPE]):\n",
    "    def calculate_value(\n",
    "        self, context: Context, legacy_result: RegressionQualityMetricResults, render: List[BaseWidgetInfo]\n",
    "    ):\n",
    "        return (\n",
    "            self.result(legacy_result.current.mean_abs_perc_error, legacy_result.current.abs_perc_error_std),\n",
    "            None\n",
    "            if legacy_result.reference is None\n",
    "            else self.result(legacy_result.reference.mean_abs_perc_error, legacy_result.reference.abs_perc_error_std),\n",
    "        )\n",
    "\n",
    "    def display_name(self) -> str:\n",
    "        return \"Mean Absolute Percentage Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b3beb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Score(SingleValueMetric):\n",
    "    error_distr: bool = False\n",
    "    error_normality: bool = False\n",
    "\n",
    "    def _default_tests(self, context: Context) -> List[BoundTest]:\n",
    "        return [gt(0).bind_single(self.get_fingerprint())]\n",
    "\n",
    "    def _default_tests_with_reference(self, context: Context) -> List[BoundTest]:\n",
    "        return [eq(Reference(relative=0.1)).bind_single(self.get_fingerprint())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd318049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2ScoreCalculation(LegacyRegressionSingleValueMetric[R2Score]):\n",
    "    def calculate_value(\n",
    "        self, context: Context, legacy_result: RegressionQualityMetricResults, render: List[BaseWidgetInfo]\n",
    "    ):\n",
    "        return (\n",
    "            self.result(legacy_result.current.r2_score),\n",
    "            None if legacy_result.reference is None else self.result(legacy_result.reference.r2_score),\n",
    "        )\n",
    "\n",
    "    def display_name(self) -> str:\n",
    "        return \"R2 Score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "130305f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsMaxError(SingleValueMetric):\n",
    "    error_distr: bool = False\n",
    "    error_normality: bool = False\n",
    "\n",
    "    def _default_tests_with_reference(self, context: Context) -> List[BoundTest]:\n",
    "        return [eq(Reference(relative=0.1)).bind_single(self.get_fingerprint())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51e72cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsMaxErrorCalculation(LegacyRegressionSingleValueMetric[AbsMaxError]):\n",
    "    def calculate_value(\n",
    "        self, context: Context, legacy_result: RegressionQualityMetricResults, render: List[BaseWidgetInfo]\n",
    "    ):\n",
    "        return (\n",
    "            self.result(legacy_result.current.abs_error_max),\n",
    "            None if legacy_result.reference is None else self.result(legacy_result.reference.abs_error_max),\n",
    "        )\n",
    "\n",
    "    def display_name(self) -> str:\n",
    "        return \"Absolute Max Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fbe025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegacyRegressionDummyMeanStdMetric(\n",
    "    MeanStdCalculation[TMeanStdMetric],\n",
    "    LegacyMetricCalculation[MeanStdValue, TMeanStdMetric, RegressionDummyMetricResults, RegressionDummyMetric],\n",
    "    Generic[TMeanStdMetric],\n",
    "    abc.ABC,\n",
    "):\n",
    "    def legacy_metric(self) -> RegressionDummyMetric:\n",
    "        return RegressionDummyMetric()\n",
    "\n",
    "    def _gen_input_data(self, context: \"Context\") -> InputData:\n",
    "        return _gen_regression_input_data(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f5fde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegacyRegressionDummyValueMetric(\n",
    "    SingleValueCalculation[TSingleValueMetric],\n",
    "    LegacyMetricCalculation[SingleValue, TSingleValueMetric, RegressionDummyMetricResults, RegressionDummyMetric],\n",
    "    Generic[TSingleValueMetric],\n",
    "    abc.ABC,\n",
    "):\n",
    "    def legacy_metric(self) -> RegressionDummyMetric:\n",
    "        return RegressionDummyMetric()\n",
    "\n",
    "    def _gen_input_data(self, context: \"Context\") -> InputData:\n",
    "        return _gen_regression_input_data(context)\n",
    "\n",
    "\n",
    "class DummyMAE(SingleValueMetric):\n",
    "    pass\n",
    "\n",
    "\n",
    "class DummyMAECalculation(LegacyRegressionDummyValueMetric[DummyMAE]):\n",
    "    def calculate_value(\n",
    "        self,\n",
    "        context: \"Context\",\n",
    "        legacy_result: RegressionDummyMetricResults,\n",
    "        render: List[BaseWidgetInfo],\n",
    "    ) -> SingleValue:\n",
    "        if legacy_result.mean_abs_error is None:\n",
    "            raise ValueError(\"No mean absolute error was calculated\")\n",
    "        return self.result(legacy_result.mean_abs_error)\n",
    "\n",
    "    def display_name(self) -> str:\n",
    "        return \"Dummy Mean Absolute Error\"\n",
    "\n",
    "\n",
    "class DummyMAPE(SingleValueMetric):\n",
    "    pass\n",
    "\n",
    "\n",
    "class DummyMAPECalculation(LegacyRegressionDummyValueMetric[DummyMAPE]):\n",
    "    def calculate_value(\n",
    "        self,\n",
    "        context: \"Context\",\n",
    "        legacy_result: RegressionDummyMetricResults,\n",
    "        render: List[BaseWidgetInfo],\n",
    "    ) -> SingleValue:\n",
    "        if legacy_result.mean_abs_perc_error is None:\n",
    "            raise ValueError(\"No mean absolute percentage error was calculated\")\n",
    "        return self.result(legacy_result.mean_abs_perc_error)\n",
    "\n",
    "    def display_name(self) -> str:\n",
    "        return \"Dummy Mean Absolute Percentage Error\"\n",
    "\n",
    "\n",
    "class DummyRMSE(SingleValueMetric):\n",
    "    pass\n",
    "\n",
    "\n",
    "class DummyRMSECalculation(LegacyRegressionDummyValueMetric[DummyRMSE]):\n",
    "    def calculate_value(\n",
    "        self,\n",
    "        context: \"Context\",\n",
    "        legacy_result: RegressionDummyMetricResults,\n",
    "        render: List[BaseWidgetInfo],\n",
    "    ) -> SingleValue:\n",
    "        if legacy_result.rmse is None:\n",
    "            raise ValueError(\"No RMSE was calculated\")\n",
    "        return self.result(legacy_result.rmse)\n",
    "\n",
    "    def display_name(self) -> str:\n",
    "        return \"Dummy RMSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee3ac3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
